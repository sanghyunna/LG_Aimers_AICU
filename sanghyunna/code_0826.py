# -*- coding: utf-8 -*-
"""code_0825.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rdYAsa6_Sj-G8BUzLJZa7eUTOrKTNxRu
"""

# 문자열 전처리 함수 정의
def tailing_zero_remover(input_string):
    # 문자열을 '-'로 분리
    parts = input_string.split('-')

    # '-'로 분리된 두 번째 부분을 정수로 변환하여 앞의 0을 제거
    parts[1] = str(int(parts[1]))

    # 다시 합쳐서 반환
    return '-'.join(parts)

def process_workorder(df):
    # 모든 문자열 열을 문자열로 변환하고 결측값을 'NaN'으로 채우기
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = df[col].astype(str).fillna('NaN')

    # 각 칼럼에 전처리 함수 적용
    df['Workorder_Fill1'] = df['Workorder_Fill1'].apply(tailing_zero_remover)
    df['Workorder_Fill2'] = df['Workorder_Fill2'].apply(tailing_zero_remover)
    df['Workorder_Dam'] = df['Workorder_Dam'].apply(tailing_zero_remover)
    df['Workorder_AutoClave'] = df['Workorder_AutoClave'].apply(tailing_zero_remover)

    # 전처리 후 각 칼럼에 동일한 문자열 추출 작업 적용
    df['Workorder_Fill1_1'] = df['Workorder_Fill1'].str[0:2]  # 0번째와 1번째 문자
    df['Workorder_Fill1_2'] = df['Workorder_Fill1'].str[2:4]    # 2번째와 3번째 문자
    df['Workorder_Fill1_3'] = df['Workorder_Fill1'].str[9:10] # 9번째 문자

    df['Workorder_Fill2_1'] = df['Workorder_Fill2'].str[0:2]  # 0번째와 1번째 문자
    df['Workorder_Fill2_2'] = df['Workorder_Fill2'].str[2:4]    # 2번째와 3번째 문자
    df['Workorder_Fill2_3'] = df['Workorder_Fill2'].str[9:10] # 9번째 문자

    df['Workorder_Dam_1'] = df['Workorder_Dam'].str[0:2]    # 0번째와 1번째 문자
    df['Workorder_Dam_2'] = df['Workorder_Dam'].str[2:4]      # 2번째와 3번째 문자
    df['Workorder_Dam_3'] = df['Workorder_Dam'].str[9:10]   # 9번째 문자

    df['Workorder_AutoClave_1'] = df['Workorder_AutoClave'].str[0:2]  # 0번째와 1번째 문자
    df['Workorder_AutoClave_2'] = df['Workorder_AutoClave'].str[2:4]    # 2번째와 3번째 문자
    df['Workorder_AutoClave_3'] = df['Workorder_AutoClave'].str[9:10] # 9번째 문자

    # 기존 칼럼 삭제
    df.drop(columns=['Workorder_Fill1', 'Workorder_Fill2', 'Workorder_Dam', 'Workorder_AutoClave'], inplace=True)

    # 새로 생성된 칼럼들을 categorical 타입으로 변환
    categorical_cols = [
        'Workorder_Fill1_1', 'Workorder_Fill1_2', 'Workorder_Fill1_3',
        'Workorder_Fill2_1', 'Workorder_Fill2_2', 'Workorder_Fill2_3',
        'Workorder_Dam_1', 'Workorder_Dam_2', 'Workorder_Dam_3',
        'Workorder_AutoClave_1', 'Workorder_AutoClave_2', 'Workorder_AutoClave_3'
    ]

    for col in categorical_cols:
        if col in df.columns:
            df[col] = df[col].astype('object')  # 먼저 'object' 타입으로 변환

    # 결측값 처리
    df = df.fillna('NaN')

    # 이제 'object' 타입 열을 'category' 타입으로 변환
    for col in categorical_cols:
        if col in df.columns:
            df[col] = df[col].astype('category')

    return df



"""# 제품 이상여부 판별 프로젝트

## 1. 데이터 불러오기

### 필수 라이브러리
"""

import os
from pprint import pprint

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score,
)
from sklearn.model_selection import train_test_split
from tqdm import tqdm

"""### 데이터 읽어오기

"""

# ROOT_DIR = "data"
ROOT_DIR = ""
RANDOM_STATE = 110

# Load data
train_data = pd.read_csv(os.path.join(ROOT_DIR, "train.csv"))

train_data.info()

train_data.describe()

train_data["target"].value_counts()

test_data = pd.read_csv(os.path.join(ROOT_DIR, "test.csv"))
test_data

test_data.info()

# 'tt' 칼럼 추가
train_data['tt'] = 'train'
test_data['tt'] = 'test'

train_target = train_data['target']
test_id = test_data['Set ID']
test_target = test_data['target']

test_data = test_data.drop(columns=['Set ID', 'target'])

train_data = train_data.drop(columns = ['target'])

integ_df = pd.concat([train_data, test_data], ignore_index=True)

integ_df.describe()

integ_df.isnull().sum()

# 각 열별 결측치 개수 확인
missing_values_count = (integ_df.isnull().sum())
missing_values_count = missing_values_count/integ_df.shape[0]
missing_values_ratio = missing_values_count * 100
print(missing_values_ratio)

bins = [0, 20, 40, 60, 70, 80, 90,100]
missing_values_counts = pd.cut(missing_values_ratio, bins=bins).value_counts().sort_index()

print(missing_values_counts)

# 결측치 비율이 90 이상인 열 삭제
threshold = 90
integ_df= integ_df.drop(columns=missing_values_ratio[missing_values_ratio >= threshold].index)

"""--"""

integ_df.info()

# 1. 상수 열 식별
constant_columns = [col for col in integ_df.columns if integ_df[col].nunique() == 1]

# 2. 상수 열 개수와 이름 확인
print(f"Number of constant columns: {len(constant_columns)}")
print(f"Constant columns: {constant_columns}")

print(integ_df['Wip Line_Dam'])

# 3. 상수 열 삭제
integ_df.drop(columns=constant_columns, inplace=True)

# 4. 삭제된 열의 이름 반환
print(f"Deleted columns: {constant_columns}")

integ_df.info()

integ_df.shape

# 숫자형 데이터만 선택
numeric_df = integ_df.select_dtypes(include=[float, int])

# 상관관계 계산
correlation_matrix = numeric_df.corr()

# 상관관계 행렬 출력
print(correlation_matrix)

!pip install seaborn

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Heatmap')
plt.show()

# 상관계수가 0.9 이상인 피처 식별
upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))
high_corr_features  = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]

print(f"Number of features with correlation higher than 0.9: {len(high_corr_features)}")
print(f"Features with high correlation: {high_corr_features}")

# 상관계수가 높은 피처 삭제
integ_df_reduced = integ_df.drop(columns=high_corr_features)

# print(f"Deleted columns due to high correlation: {high_corr_features}")
# print(df_reduced.head())

# # 삭제할 컬럼의 리스트 확인
# print(f"Columns to be deleted: {high_corr_features}")

# # 삭제할 컬럼의 개수 확인
# print(f"Number of columns to delete: {len(high_corr_features)}")

# # df_reduced의 컬럼 개수 확인
# print(f"Number of columns in df_reduced after deletion: {df_reduced.shape[1]}")

# # 삭제 후 컬럼 목록 확인
# print(f"Columns remaining in df_reduced: {df_reduced.columns.tolist()}")

# df_reduced.shape

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# df_reduced에서 숫자형 데이터만 선택하여 상관계수 행렬 계산
# 히트맵 생성
plt.figure(figsize=(12, 10))  # 히트맵 크기 설정
sns.heatmap(integ_df_reduced.select_dtypes(include=['float', 'int']).corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Heatmap of df_reduced')
plt.show()

#스케일링 진행
from sklearn.preprocessing import StandardScaler, MinMaxScaler

scaler = StandardScaler()
numeric_df_scaled = scaler.fit_transform(numeric_df)

integ_df

# 범주형 데이터 열 확인
categorical_columns = integ_df_reduced.select_dtypes(include=['object', 'category']).columns

# 범주형 데이터 열 개수 출력
print(f"Number of categorical columns: {len(categorical_columns)}")
print(f"Categorical columns: {categorical_columns.tolist()}")

# 범주형 열만 선택하여 새로운 DataFrame 생성
categorical_df = integ_df_reduced[['Equipment_Dam', 'Model.Suffix_Dam', 'Workorder_Dam',
                     'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam',
                     'Model.Suffix_AutoClave', 'Workorder_AutoClave',
                     'Chamber Temp. Judge Value_AutoClave', 'Equipment_Fill1',
                     'Model.Suffix_Fill1', 'Workorder_Fill1',
                     'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1',
                     'Equipment_Fill2', 'Model.Suffix_Fill2', 'Workorder_Fill2',
                     'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2']]

# 선택된 범주형 열의 데이터 출력
print(categorical_df.head())

# 각 범주형 열에서 고유 값의 개수와 고유 값 출력
for col in categorical_columns:
    num_unique_vals = len(integ_df_reduced[col].unique())  # 고유 값의 개수
    print(f"Column: {col}")
    print(f"Number of unique values: {num_unique_vals}")
    print(f"Unique values: {integ_df_reduced[col].unique()}\n")  # 고유 값 출력

"""-----"""

integ_df_reduced.shape

import pandas as pd

integ_df_reduced = process_workorder(integ_df_reduced)

# 'tt' 칼럼을 기준으로 train과 test로 다시 분리
train = integ_df_reduced[integ_df_reduced['tt'] == 'train'].copy()
test = integ_df_reduced[integ_df_reduced['tt'] == 'test'].copy()

# 'tt' 칼럼은 이제 필요 없으므로 삭제
train.drop(columns=['tt'], inplace=True)
test.drop(columns=['tt'], inplace=True)

# train 데이터에 원래 target 값 추가
train['target'] = train_target.values

# test 데이터에 NaN으로 채워진 target 칼럼 추가
test['target'] = float('nan')

# 결과 확인
print("Train data shape:", train.shape)
print("Test data shape:", test.shape)
print(train.head())
print(test.head())

test

train

test.shape



normal_ratio = 1.0  # 1.0 means 1:1 ratio

df_normal = train[train["target"] == "Normal"]
df_abnormal = train[train["target"] == "AbNormal"]

print(f"  Total: Normal: {len(df_normal)}, AbNormal: {len(df_abnormal)}")

df_normal = df_normal.sample(n=int(len(df_abnormal) * normal_ratio), replace=False, random_state=RANDOM_STATE)
train_df = pd.concat([df_normal, df_abnormal], axis=0).reset_index(drop=True)
train_df.value_counts("target")

# 레이블 인코딩: 'Normal'을 1로, 'Abnormal'을 0으로 변환
mapping = {'Normal': 1, 'AbNormal': 0}
train_df['target'] = train_df['target'].map(mapping)

X = train_df.drop('target', axis=1)
y = train_df['target']

# for c in sorted(X.columns.tolist()):
#   print(c)

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.isnull().sum()

!pip install catboost

!pip install optuna

from catboost import CatBoostClassifier
import optuna
import numpy as np
from sklearn.metrics import f1_score
from sklearn.preprocessing import LabelEncoder

# 모든 범주형 열의 NaN 값을 'NaN' 문자열로 대체
def preprocess_data(X_train, X_val):
    categorical_columns = X_train.select_dtypes(include=['object', 'category']).columns

    for col in categorical_columns:
        X_train[col] = X_train[col].astype(str).fillna('NaN')
        X_val[col] = X_val[col].astype(str).fillna('NaN')

    return X_train, X_val

# 데이터 전처리
X_train, X_val = preprocess_data(X_train, X_val)


# 범주형 변수 인덱스 찾기
categorical_features_indices = [i for i, col in enumerate(X_train.columns) if X_train[col].dtype.name == 'category']

# Optuna 목적 함수 정의
def objective(trial):
    # 하이퍼파라미터 탐색 공간
    iterations = trial.suggest_int('iterations', 500, 2000)
    learning_rate = trial.suggest_float('learning_rate', 1e-3, 0.3, log=True)
    depth = trial.suggest_int('depth', 4, 10)
    l2_leaf_reg = trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True)

    model = CatBoostClassifier(
        iterations=iterations,
        learning_rate=learning_rate,
        depth=depth,
        l2_leaf_reg=l2_leaf_reg,
        eval_metric='F1',
        random_seed=42,
        logging_level='Silent'
    )

    # 모델 학습
    model.fit(
        X_train,
        y_train,
        cat_features=categorical_features_indices,  # 범주형 열의 인덱스 전달
        eval_set=(X_val, y_val),
        early_stopping_rounds=100,
        verbose=False
    )

    # 모델 예측
    y_pred_encoded = model.predict(X_val)

    # F1 점수 반환
    return f1_score(y_val, y_pred_encoded)

# Optuna study 설정 및 최적화
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)

# 최적의 하이퍼파라미터 출력
print(f"Best trial: {study.best_trial.params}")

# 최적의 하이퍼파라미터로 최종 모델 학습
best_params = study.best_trial.params
model = CatBoostClassifier(
    iterations=best_params['iterations'],
    learning_rate=best_params['learning_rate'],
    depth=best_params['depth'],
    l2_leaf_reg=best_params['l2_leaf_reg'],
    eval_metric='F1',
    random_seed=42
)

model.fit(
    X_train,
    y_train,
    cat_features=categorical_features_indices,
    eval_set=(X_val, y_val),
    verbose=100,
    early_stopping_rounds=100
)

# 최종 모델로 예측
y_pred_encoded = model.predict(X_val)

# F1 점수 계산
f1 = f1_score(y_val, y_pred_encoded)
print(f"Final F1 score: {f1}")

# 피처 중요도 시각화
feature_importances = model.get_feature_importance()
feature_names = X_train.columns

# 중요도를 데이터프레임으로 정리
importances_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False) # 중요도 순으로 정렬


# 시각화
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=importances_df)
plt.title('Feature Importance')
plt.show()

train.shape

test.shape

# 모든 범주형 열의 NaN 값을 'NaN' 문자열로 대체 (X_test 포함)
# categorical_columns = test.select_dtypes(include=['object', 'category']).columns

# for col in categorical_columns:
#     test[col] = test[col].astype(str).fillna('NaN')

# exclude_columns = ['Workorder_first', 'Workorder_sec', 'Workorder_third']
# test['Workorder_third']

# 이후에 모델 예측 수행
test_pred = model.predict(test)

test.shape

# 레이블 인코딩
mapping = {1: 'Normal', 0: 'AbNormal'}
test_pred = np.vectorize(mapping.get)(test_pred)

# submission 파일 호출
df_sub = pd.read_csv("submission.csv")
df_sub["target"] = test_pred

# 제출 파일 저장
df_sub.to_csv("submission.csv", index=False)



