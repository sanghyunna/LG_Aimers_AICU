{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas scikit-learn catboost matplotlib seaborn optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G60gMu0ErDEH",
        "outputId": "cd8aaaab-0319-4a14-df09-c06848294b47"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.10/dist-packages (1.2.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.4)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (1.13.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.32)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.5)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (9.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from catboost import CatBoostClassifier, Pool\n",
        "import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pickle"
      ],
      "metadata": {
        "id": "9FZkTfx7rEjg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리 함수\n",
        "def process_workorder(df):\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        df[col] = df[col].astype(str).fillna('NaN')\n",
        "\n",
        "    df['Workorder_Fill1'] = df['Workorder_Fill1'].apply(tailing_zero_remover)\n",
        "    df['Workorder_Fill2'] = df['Workorder_Fill2'].apply(tailing_zero_remover)\n",
        "    df['Workorder_Dam'] = df['Workorder_Dam'].apply(tailing_zero_remover)\n",
        "    df['Workorder_AutoClave'] = df['Workorder_AutoClave'].apply(tailing_zero_remover)\n",
        "\n",
        "    df['Workorder_Fill1_1'] = df['Workorder_Fill1'].str[0:2]\n",
        "    df['Workorder_Fill1_2'] = df['Workorder_Fill1'].str[2:4]\n",
        "    df['Workorder_Fill1_3'] = df['Workorder_Fill1'].str[9:10]\n",
        "\n",
        "    df['Workorder_Fill2_1'] = df['Workorder_Fill2'].str[0:2]\n",
        "    df['Workorder_Fill2_2'] = df['Workorder_Fill2'].str[2:4]\n",
        "    df['Workorder_Fill2_3'] = df['Workorder_Fill2'].str[9:10]\n",
        "\n",
        "    df['Workorder_Dam_1'] = df['Workorder_Dam'].str[0:2]\n",
        "    df['Workorder_Dam_2'] = df['Workorder_Dam'].str[2:4]\n",
        "    df['Workorder_Dam_3'] = df['Workorder_Dam'].str[9:10]\n",
        "\n",
        "    df['Workorder_AutoClave_1'] = df['Workorder_AutoClave'].str[0:2]\n",
        "    df['Workorder_AutoClave_2'] = df['Workorder_AutoClave'].str[2:4]\n",
        "    df['Workorder_AutoClave_3'] = df['Workorder_AutoClave'].str[9:10]\n",
        "\n",
        "    df.drop(columns=['Workorder_Fill1', 'Workorder_Fill2', 'Workorder_Dam', 'Workorder_AutoClave'], inplace=True)\n",
        "\n",
        "    categorical_cols = [\n",
        "        'Workorder_Fill1_1', 'Workorder_Fill1_2', 'Workorder_Fill1_3',\n",
        "        'Workorder_Fill2_1', 'Workorder_Fill2_2', 'Workorder_Fill2_3',\n",
        "        'Workorder_Dam_1', 'Workorder_Dam_2', 'Workorder_Dam_3',\n",
        "        'Workorder_AutoClave_1', 'Workorder_AutoClave_2', 'Workorder_AutoClave_3'\n",
        "    ]\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype('object')\n",
        "\n",
        "    df = df.fillna('NaN')\n",
        "\n",
        "    for col in categorical_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype('category')\n",
        "\n",
        "    def convert_to_bool(series):\n",
        "        tt_col = ['train', 'test']\n",
        "        unique_values = series.unique()\n",
        "        if len(unique_values) != 2:\n",
        "            return series\n",
        "        elif unique_values[0] in tt_col and unique_values[1] in tt_col:\n",
        "            return series\n",
        "        mapping = {unique_values[0]: False, unique_values[1]: True}\n",
        "        return series.map(mapping)\n",
        "\n",
        "    for col in df.columns:\n",
        "        if df[col].nunique() == 2:\n",
        "            df[col] = convert_to_bool(df[col])\n",
        "\n",
        "    return df\n",
        "\n",
        "# 문자열 전처리 함수\n",
        "def tailing_zero_remover(input_string):\n",
        "    parts = input_string.split('-')\n",
        "    parts[1] = str(int(parts[1]))\n",
        "    return '-'.join(parts)"
      ],
      "metadata": {
        "id": "Fw0YLgqQrKyp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 로딩\n",
        "ROOT_DIR = \"./\"\n",
        "RANDOM_STATE = 110\n",
        "\n",
        "train_data = pd.read_csv(os.path.join(ROOT_DIR, \"train.csv\"))\n",
        "test_data = pd.read_csv(os.path.join(ROOT_DIR, \"test.csv\"))\n",
        "\n",
        "# 통합 df 생성\n",
        "train_data['tt'] = 'train'\n",
        "test_data['tt'] = 'test'\n",
        "\n",
        "train_target = train_data['target']\n",
        "test_id = test_data['Set ID']\n",
        "test_target = test_data['target']\n",
        "\n",
        "test_data = test_data.drop(columns=['Set ID', 'target'])\n",
        "train_data = train_data.drop(columns = ['target'])\n",
        "\n",
        "integ_df = pd.concat([train_data, test_data], ignore_index=True)"
      ],
      "metadata": {
        "id": "kmCM06b_rM9C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치 비율이 90% 이상인 열 삭제\n",
        "threshold = 90\n",
        "missing_values_ratio = (integ_df.isnull().sum() / integ_df.shape[0]) * 100\n",
        "integ_df= integ_df.drop(columns=missing_values_ratio[missing_values_ratio >= threshold].index)\n",
        "\n",
        "# 상수 열 삭제\n",
        "constant_columns = [col for col in integ_df.columns if integ_df[col].nunique() == 1]\n",
        "integ_df.drop(columns=constant_columns, inplace=True)\n",
        "\n",
        "# 상관계수가 높은 피처 삭제\n",
        "numeric_df = integ_df.select_dtypes(include=[float, int])\n",
        "correlation_matrix = numeric_df.corr()\n",
        "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
        "high_corr_features  = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]\n",
        "integ_df_reduced = integ_df.drop(columns=high_corr_features)"
      ],
      "metadata": {
        "id": "eOXD5LR7rOnk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 범주형 열 처리 및 스케일링\n",
        "def preprocess_data(df):\n",
        "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
        "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "    # 범주형 변수 원핫 인코딩\n",
        "    df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "    # 수치형 변수 스케일링\n",
        "    scaler = StandardScaler()\n",
        "    df[numeric_columns] = scaler.fit_transform(df[numeric_columns])\n",
        "\n",
        "    return df\n",
        "\n",
        "integ_df_reduced = preprocess_data(integ_df_reduced)"
      ],
      "metadata": {
        "id": "0UQi1F4prP-3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 분리 및 타겟 인코딩\n",
        "train = integ_df_reduced[integ_df_reduced['tt_train'] == True].copy()\n",
        "test = integ_df_reduced[integ_df_reduced['tt_train'] == False].copy()\n",
        "train.drop(columns=['tt_train'], inplace=True)\n",
        "test.drop(columns=['tt_train'], inplace=True)\n",
        "train['target'] = train_target.values\n",
        "\n",
        "# 불균형 데이터 처리\n",
        "normal_ratio = 2.0\n",
        "df_normal = train[train[\"target\"] == \"Normal\"]\n",
        "df_abnormal = train[train[\"target\"] == \"AbNormal\"]\n",
        "df_normal = df_normal.sample(n=int(len(df_abnormal) * normal_ratio), replace=False, random_state=RANDOM_STATE)\n",
        "train_df = pd.concat([df_normal, df_abnormal], axis=0).reset_index(drop=True)\n",
        "\n",
        "# 타겟 인코딩\n",
        "mapping = {'Normal': 1, 'AbNormal': 0}\n",
        "train_df['target'] = train_df['target'].map(mapping)\n",
        "X = train_df.drop('target', axis=1)\n",
        "y = train_df['target']\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "9yDbtzJ6rSZx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 범주형 변수 인덱스 찾기 (Catboost에서 사용)\n",
        "categorical_features_indices = [\n",
        "    i for i, col in enumerate(X_train.columns)\n",
        "    if X_train[col].dtype.name in ['category', 'object']\n",
        "]"
      ],
      "metadata": {
        "id": "wXudjyq-raes"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CatBoost 및 Logistic Regression의 Optuna 하이퍼파라미터 최적화\n",
        "def objective_catboost(trial):\n",
        "    iterations = trial.suggest_int('iterations', 500, 1000)\n",
        "    learning_rate = trial.suggest_float('learning_rate', 1e-3, 0.3, log=True)\n",
        "    depth = trial.suggest_int('depth', 4, 10)\n",
        "    l2_leaf_reg = trial.suggest_float('l2_leaf_reg', 1e-2, 100.0, log=True)\n",
        "    rsm = trial.suggest_float('rsm', 0.5, 1.0) ##\n",
        "    border_count = trial.suggest_int('border_count', 32, 255)\n",
        "\n",
        "    model = CatBoostClassifier(\n",
        "        iterations=iterations,\n",
        "        learning_rate=learning_rate,\n",
        "        depth=depth,\n",
        "        l2_leaf_reg=l2_leaf_reg,\n",
        "        rsm=rsm, ##\n",
        "        border_count=border_count,\n",
        "        eval_metric='F1',\n",
        "        random_seed=42,\n",
        "        logging_level='Silent',\n",
        "        thread_count=-1,\n",
        "        task_type=\"CPU\",\n",
        "    )\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    f1_scores = []\n",
        "\n",
        "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
        "        X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "        train_pool = Pool(data=X_fold_train, label=y_fold_train, cat_features=categorical_features_indices)\n",
        "        val_pool = Pool(data=X_fold_val, label=y_fold_val, cat_features=categorical_features_indices)\n",
        "        model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=100, verbose=False)\n",
        "        y_pred_encoded = model.predict(X_fold_val)\n",
        "        fold_f1_score = f1_score(y_fold_val, y_pred_encoded)\n",
        "        f1_scores.append(fold_f1_score)\n",
        "\n",
        "    return np.mean(f1_scores)"
      ],
      "metadata": {
        "id": "Kfaig1Strek-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "catboost_model = None\n",
        "if os.path.exists(os.path.join(ROOT_DIR, 'best_catboost_model.pkl')):\n",
        "  with open(os.path.join(ROOT_DIR, 'best_catboost_model.pkl'), 'rb') as f:\n",
        "      catboost_model = pickle.load(f)\n",
        "else:\n",
        "  study_catboost = optuna.create_study(direction='maximize')\n",
        "  study_catboost.optimize(objective_catboost, n_trials=50)\n",
        "  best_params_catboost = study_catboost.best_trial.params\n",
        "\n",
        "  # 최적의 CatBoost 모델\n",
        "  catboost_model = CatBoostClassifier(\n",
        "      iterations=best_params_catboost['iterations'],\n",
        "      learning_rate=best_params_catboost['learning_rate'],\n",
        "      depth=best_params_catboost['depth'],\n",
        "      l2_leaf_reg=best_params_catboost['l2_leaf_reg'],\n",
        "      rsm=best_params_catboost['rsm'], ##\n",
        "      border_count=best_params_catboost['border_count'],\n",
        "      eval_metric='F1',\n",
        "      random_seed=42,\n",
        "      cat_features=categorical_features_indices,\n",
        "      verbose=100,\n",
        "      early_stopping_rounds=100\n",
        "  )\n",
        "  catboost_model.fit(X_train, y_train, cat_features=categorical_features_indices)\n",
        "  # 모델을 pkl 파일로 저장\n",
        "  with open(os.path.join(ROOT_DIR, 'best_catboost_model.pkl'), 'wb') as f:\n",
        "      pickle.dump(catboost_model, f)"
      ],
      "metadata": {
        "id": "bLZBQq0HrgKj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression 최적화 함수\n",
        "def objective_logistic(trial):\n",
        "    # C 파라미터를 최적화\n",
        "    C = trial.suggest_float('C', 1e-4, 1e2, log=True)\n",
        "\n",
        "    # Logistic Regression 모델 정의\n",
        "    model = make_pipeline(\n",
        "        StandardScaler(),  # 데이터 스케일링 추가\n",
        "        LogisticRegression(\n",
        "            penalty='l2',               # L2 정규화 사용\n",
        "            C=C,\n",
        "            solver='saga',              # saga 솔버 사용\n",
        "            class_weight='balanced',    # 불균형 데이터셋에 대해 클래스 가중치 적용\n",
        "            random_state=42,\n",
        "            max_iter=1000\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # 교차 검증 설정\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    f1_scores = []\n",
        "\n",
        "    # 교차 검증 루프\n",
        "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
        "        X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        model.fit(X_fold_train, y_fold_train)\n",
        "        y_pred_encoded = model.predict(X_fold_val)\n",
        "        fold_f1_score = f1_score(y_fold_val, y_pred_encoded)\n",
        "        f1_scores.append(fold_f1_score)\n",
        "\n",
        "    return np.mean(f1_scores)"
      ],
      "metadata": {
        "id": "4JGK0Lr0riAO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression 모델 최적화 및 학습\n",
        "logistic_model = None\n",
        "if os.path.exists(os.path.join(ROOT_DIR, 'best_logistic_model.pkl')):\n",
        "    with open(os.path.join(ROOT_DIR, 'best_logistic_model.pkl'), 'rb') as f:\n",
        "        logistic_model = pickle.load(f)\n",
        "else:\n",
        "    study_logistic = optuna.create_study(direction='maximize')\n",
        "    study_logistic.optimize(objective_logistic, n_trials=50)\n",
        "    best_params_logistic = study_logistic.best_trial.params\n",
        "\n",
        "    # 최적의 Logistic Regression 모델\n",
        "    logistic_model = make_pipeline(\n",
        "        StandardScaler(),  # 데이터 스케일링 추가\n",
        "        LogisticRegression(\n",
        "            penalty='l2',\n",
        "            C=best_params_logistic['C'],\n",
        "            solver='saga',\n",
        "            class_weight='balanced',\n",
        "            random_state=42,\n",
        "            max_iter=1000,\n",
        "        )\n",
        "    )\n",
        "    logistic_model.fit(X_train, y_train)\n",
        "\n",
        "    # 모델을 pkl 파일로 저장\n",
        "    with open(os.path.join(ROOT_DIR, 'best_logistic_model.pkl'), 'wb') as f:\n",
        "        pickle.dump(logistic_model, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrgJ8p_zrj0r",
        "outputId": "e86fc070-51eb-4f15-a75a-bf2bd7a8c185"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-08-29 06:51:44,975] A new study created in memory with name: no-name-4899c431-60a8-4a68-ad1a-b9085108b7eb\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2024-08-29 07:07:09,324] Trial 0 finished with value: 0.6695018665883444 and parameters: {'C': 14.807367075125669}. Best is trial 0 with value: 0.6695018665883444.\n",
            "[I 2024-08-29 07:07:57,817] Trial 1 finished with value: 0.6778561887845984 and parameters: {'C': 0.00026115726041615504}. Best is trial 1 with value: 0.6778561887845984.\n",
            "[I 2024-08-29 07:08:46,047] Trial 2 finished with value: 0.6776548699321026 and parameters: {'C': 0.0002409221092742596}. Best is trial 1 with value: 0.6778561887845984.\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2024-08-29 07:24:04,831] Trial 3 finished with value: 0.6690983960046315 and parameters: {'C': 0.49921873096066466}. Best is trial 1 with value: 0.6778561887845984.\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2024-08-29 07:39:19,512] Trial 4 finished with value: 0.6695018665883444 and parameters: {'C': 41.584549161371186}. Best is trial 1 with value: 0.6778561887845984.\n",
            "[I 2024-08-29 07:40:07,559] Trial 5 finished with value: 0.6818889560273685 and parameters: {'C': 0.00011268738454558689}. Best is trial 5 with value: 0.6818889560273685.\n",
            "[I 2024-08-29 07:42:38,026] Trial 6 finished with value: 0.6668207389001521 and parameters: {'C': 0.003510665637344045}. Best is trial 5 with value: 0.6818889560273685.\n",
            "[I 2024-08-29 07:43:23,344] Trial 7 finished with value: 0.6821594039023976 and parameters: {'C': 0.00010719620139788393}. Best is trial 7 with value: 0.6821594039023976.\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2024-08-29 07:58:36,785] Trial 8 finished with value: 0.6687955198877622 and parameters: {'C': 0.2538442446439827}. Best is trial 7 with value: 0.6821594039023976.\n",
            "[I 2024-08-29 08:07:05,187] Trial 9 finished with value: 0.6663031795743326 and parameters: {'C': 0.017951706582392625}. Best is trial 7 with value: 0.6821594039023976.\n",
            "[I 2024-08-29 08:10:24,484] Trial 10 finished with value: 0.6656279750268427 and parameters: {'C': 0.004989868256402264}. Best is trial 7 with value: 0.6821594039023976.\n",
            "[I 2024-08-29 08:11:11,797] Trial 11 finished with value: 0.6819669145099295 and parameters: {'C': 0.00012846287680438783}. Best is trial 7 with value: 0.6821594039023976.\n",
            "[I 2024-08-29 08:12:47,855] Trial 12 finished with value: 0.6674961103385009 and parameters: {'C': 0.0019240725200165983}. Best is trial 7 with value: 0.6821594039023976.\n",
            "[I 2024-08-29 08:13:56,211] Trial 13 finished with value: 0.6711653739658269 and parameters: {'C': 0.0010657688251934927}. Best is trial 7 with value: 0.6821594039023976.\n",
            "[I 2024-08-29 08:25:46,536] Trial 14 finished with value: 0.6672179455612455 and parameters: {'C': 0.02910075578474999}. Best is trial 7 with value: 0.6821594039023976.\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2024-08-29 08:40:53,912] Trial 15 finished with value: 0.6695018665883444 and parameters: {'C': 2.422313531637314}. Best is trial 7 with value: 0.6821594039023976.\n",
            "[I 2024-08-29 08:41:49,762] Trial 16 finished with value: 0.6743196541260283 and parameters: {'C': 0.00043973865276118165}. Best is trial 7 with value: 0.6821594039023976.\n",
            "[I 2024-08-29 08:50:59,117] Trial 17 finished with value: 0.6669998445636136 and parameters: {'C': 0.020281212711425755}. Best is trial 7 with value: 0.6821594039023976.\n",
            "[I 2024-08-29 08:52:03,545] Trial 18 finished with value: 0.6715654274007951 and parameters: {'C': 0.0010055290421581058}. Best is trial 7 with value: 0.6821594039023976.\n",
            "[I 2024-08-29 08:52:48,747] Trial 19 finished with value: 0.6817870066299753 and parameters: {'C': 0.00010934856970720628}. Best is trial 7 with value: 0.6821594039023976.\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2024-08-29 09:07:58,123] Trial 20 finished with value: 0.6696953231753964 and parameters: {'C': 0.06612219252294581}. Best is trial 7 with value: 0.6821594039023976.\n",
            "[I 2024-08-29 09:08:44,943] Trial 21 finished with value: 0.6824531392919457 and parameters: {'C': 0.00012181692901242047}. Best is trial 21 with value: 0.6824531392919457.\n",
            "[I 2024-08-29 09:09:46,019] Trial 22 finished with value: 0.6720108700882601 and parameters: {'C': 0.0007903342679733429}. Best is trial 21 with value: 0.6824531392919457.\n",
            "[I 2024-08-29 09:13:23,642] Trial 23 finished with value: 0.664944021786587 and parameters: {'C': 0.005820884236731156}. Best is trial 21 with value: 0.6824531392919457.\n",
            "[I 2024-08-29 09:14:11,901] Trial 24 finished with value: 0.6824488713050993 and parameters: {'C': 0.00010284351646145088}. Best is trial 21 with value: 0.6824531392919457.\n",
            "[I 2024-08-29 09:15:05,511] Trial 25 finished with value: 0.6727039867953026 and parameters: {'C': 0.0006039974720938926}. Best is trial 21 with value: 0.6824531392919457.\n",
            "[I 2024-08-29 09:15:56,615] Trial 26 finished with value: 0.6765232163374403 and parameters: {'C': 0.00032798647391891344}. Best is trial 21 with value: 0.6824531392919457.\n",
            "[I 2024-08-29 09:17:52,279] Trial 27 finished with value: 0.6664898540453519 and parameters: {'C': 0.00249392673256332}. Best is trial 21 with value: 0.6824531392919457.\n",
            "[I 2024-08-29 09:22:36,408] Trial 28 finished with value: 0.6652090155510701 and parameters: {'C': 0.008140517970132658}. Best is trial 21 with value: 0.6824531392919457.\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2024-08-29 09:37:48,393] Trial 29 finished with value: 0.6695018665883444 and parameters: {'C': 13.714285575314733}. Best is trial 21 with value: 0.6824531392919457.\n",
            "[I 2024-08-29 09:38:36,190] Trial 30 finished with value: 0.679212161065972 and parameters: {'C': 0.0002035767989733311}. Best is trial 21 with value: 0.6824531392919457.\n",
            "[I 2024-08-29 09:39:21,422] Trial 31 finished with value: 0.6817870066299753 and parameters: {'C': 0.00010974429715817686}. Best is trial 21 with value: 0.6824531392919457.\n",
            "[I 2024-08-29 09:40:09,538] Trial 32 finished with value: 0.677566138553704 and parameters: {'C': 0.00027824090411114356}. Best is trial 21 with value: 0.6824531392919457.\n",
            "[I 2024-08-29 09:41:29,967] Trial 33 finished with value: 0.6707726884095411 and parameters: {'C': 0.0014840582982342892}. Best is trial 21 with value: 0.6824531392919457.\n",
            "[I 2024-08-29 09:42:18,983] Trial 34 finished with value: 0.679212161065972 and parameters: {'C': 0.0002010636880898107}. Best is trial 21 with value: 0.6824531392919457.\n",
            "[I 2024-08-29 09:43:12,425] Trial 35 finished with value: 0.6738313162527675 and parameters: {'C': 0.000451955232617764}. Best is trial 21 with value: 0.6824531392919457.\n",
            "[I 2024-08-29 09:44:08,259] Trial 36 finished with value: 0.6808684905288667 and parameters: {'C': 0.00017655392554305653}. Best is trial 21 with value: 0.6824531392919457.\n",
            "[I 2024-08-29 09:45:00,994] Trial 37 finished with value: 0.6730731476018887 and parameters: {'C': 0.00048130386244988325}. Best is trial 21 with value: 0.6824531392919457.\n",
            "[I 2024-08-29 09:45:48,143] Trial 38 finished with value: 0.6829472599238873 and parameters: {'C': 0.00011910479725299808}. Best is trial 38 with value: 0.6829472599238873.\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "[I 2024-08-29 10:00:56,492] Trial 39 finished with value: 0.6688988944300802 and parameters: {'C': 0.33175549955392664}. Best is trial 38 with value: 0.6829472599238873.\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 앙상블 모델 정의 (Soft Voting)\n",
        "ensemble_model = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('catboost', catboost_model),\n",
        "        ('logistic', logistic_model),\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# 앙상블 모델 학습 및 예측\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "y_val_pred = ensemble_model.predict(X_val)"
      ],
      "metadata": {
        "id": "NWhATykgrljM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# F1 점수 계산\n",
        "f1 = f1_score(y_val, y_val_pred)\n",
        "print(f\"Final F1 score with ensemble: {f1}\")\n",
        "\n",
        "# 테스트 데이터 예측\n",
        "test_pred = ensemble_model.predict(test)\n",
        "\n",
        "# 디코딩\n",
        "mapping = {1: 'Normal', 0: 'AbNormal'}\n",
        "test_pred = np.vectorize(mapping.get)(test_pred)\n",
        "\n",
        "# 제출 파일 생성\n",
        "df_sub = pd.read_csv(os.path.join(ROOT_DIR, \"submission.csv\"))\n",
        "df_sub[\"target\"] = test_pred\n",
        "df_sub.to_csv(os.path.join(ROOT_DIR, \"submission_ensemble.csv\"), index=False)"
      ],
      "metadata": {
        "id": "WFjp3Kj8rnBT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}