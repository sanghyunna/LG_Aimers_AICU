# -*- coding: utf-8 -*-
"""code_0827.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/171gpnek7rBgdwot6sb8cKc2SJLf8xQvo

# 제품 이상여부 판별 프로젝트

## 1. 데이터 불러오기

### 필수 라이브러리
"""

def process_workorder(df):
    # 모든 문자열 열을 문자열로 변환하고 결측값을 'NaN'으로 채우기
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = df[col].astype(str).fillna('NaN')

    # 각 칼럼에 전처리 함수 적용
    df['Workorder_Fill1'] = df['Workorder_Fill1'].apply(tailing_zero_remover)
    df['Workorder_Fill2'] = df['Workorder_Fill2'].apply(tailing_zero_remover)
    df['Workorder_Dam'] = df['Workorder_Dam'].apply(tailing_zero_remover)
    df['Workorder_AutoClave'] = df['Workorder_AutoClave'].apply(tailing_zero_remover)

    # 전처리 후 각 칼럼에 동일한 문자열 추출 작업 적용
    df['Workorder_Fill1_1'] = df['Workorder_Fill1'].str[0:2]  # 0번째와 1번째 문자
    df['Workorder_Fill1_2'] = df['Workorder_Fill1'].str[2:4]    # 2번째와 3번째 문자
    df['Workorder_Fill1_3'] = df['Workorder_Fill1'].str[9:10] # 9번째 문자

    df['Workorder_Fill2_1'] = df['Workorder_Fill2'].str[0:2]  # 0번째와 1번째 문자
    df['Workorder_Fill2_2'] = df['Workorder_Fill2'].str[2:4]    # 2번째와 3번째 문자
    df['Workorder_Fill2_3'] = df['Workorder_Fill2'].str[9:10] # 9번째 문자

    df['Workorder_Dam_1'] = df['Workorder_Dam'].str[0:2]    # 0번째와 1번째 문자
    df['Workorder_Dam_2'] = df['Workorder_Dam'].str[2:4]      # 2번째와 3번째 문자
    df['Workorder_Dam_3'] = df['Workorder_Dam'].str[9:10]   # 9번째 문자

    df['Workorder_AutoClave_1'] = df['Workorder_AutoClave'].str[0:2]  # 0번째와 1번째 문자
    df['Workorder_AutoClave_2'] = df['Workorder_AutoClave'].str[2:4]    # 2번째와 3번째 문자
    df['Workorder_AutoClave_3'] = df['Workorder_AutoClave'].str[9:10] # 9번째 문자

    # 기존 칼럼 삭제
    df.drop(columns=['Workorder_Fill1', 'Workorder_Fill2', 'Workorder_Dam', 'Workorder_AutoClave'], inplace=True)

    # 새로 생성된 칼럼들을 categorical 타입으로 변환
    categorical_cols = [
        'Workorder_Fill1_1', 'Workorder_Fill1_2', 'Workorder_Fill1_3',
        'Workorder_Fill2_1', 'Workorder_Fill2_2', 'Workorder_Fill2_3',
        'Workorder_Dam_1', 'Workorder_Dam_2', 'Workorder_Dam_3',
        'Workorder_AutoClave_1', 'Workorder_AutoClave_2', 'Workorder_AutoClave_3'
    ]

    for col in categorical_cols:
        if col in df.columns:
            df[col] = df[col].astype('object')  # 먼저 'object' 타입으로 변환

    # 결측값 처리
    df = df.fillna('NaN')

    # 이제 'object' 타입 열을 'category' 타입으로 변환
    for col in categorical_cols:
        if col in df.columns:
            df[col] = df[col].astype('category')

    def convert_to_bool(series):
      tt_col = ['train', 'test']
      unique_values = series.unique()
      if len(unique_values) != 2:
          return series  # 변환 필요 없는 경우 원본 시리즈 반환
      elif unique_values[0] in tt_col and unique_values[1] in tt_col:
          return series
      mapping = {unique_values[0]: False, unique_values[1]: True}
      return series.map(mapping)

    for col in df.columns:
        if df[col].nunique() == 2:
            df[col] = convert_to_bool(df[col])

    return df

# 문자열 전처리 함수 정의
def tailing_zero_remover(input_string):
    # 문자열을 '-'로 분리
    parts = input_string.split('-')

    # '-'로 분리된 두 번째 부분을 정수로 변환하여 앞의 0을 제거
    parts[1] = str(int(parts[1]))

    # 다시 합쳐서 반환
    return '-'.join(parts)

import os
from pprint import pprint

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    f1_score,
    precision_score,
    recall_score,
)
from sklearn.model_selection import train_test_split
from tqdm import tqdm

"""### 데이터 읽어오기"""

# ROOT_DIR = "data"
ROOT_DIR = "/content/drive/MyDrive/Colab Notebooks/"
RANDOM_STATE = 110

# Load data
train_data = pd.read_csv(os.path.join(ROOT_DIR, "train.csv"))

train_data.info()

train_data.describe()

train_data["target"].value_counts()

test_data = pd.read_csv(os.path.join(ROOT_DIR, "test.csv"))
test_data

test_data.info()

# 'tt' 칼럼 추가
train_data['tt'] = 'train'
test_data['tt'] = 'test'

train_target = train_data['target']
test_id = test_data['Set ID']
test_target = test_data['target']

test_data = test_data.drop(columns=['Set ID', 'target'])

train_data = train_data.drop(columns = ['target'])

integ_df = pd.concat([train_data, test_data], ignore_index=True)

integ_df.describe()

integ_df.isnull().sum()

# 각 열별 결측치 개수 확인
missing_values_count = (integ_df.isnull().sum())
missing_values_count = missing_values_count/integ_df.shape[0]
missing_values_ratio = missing_values_count * 100
print(missing_values_ratio)

bins = [0, 20, 40, 60, 70, 80, 90,100]
missing_values_counts = pd.cut(missing_values_ratio, bins=bins).value_counts().sort_index()

print(missing_values_counts)

# 결측치 비율이 90 이상인 열 삭제
threshold = 90
integ_df= integ_df.drop(columns=missing_values_ratio[missing_values_ratio >= threshold].index)

integ_df.info()

# 1. 상수 열 식별
constant_columns = [col for col in integ_df.columns if integ_df[col].nunique() == 1]

# 2. 상수 열 개수와 이름 확인
print(f"Number of constant columns: {len(constant_columns)}")
print(f"Constant columns: {constant_columns}")

print(integ_df['Wip Line_Dam'])

# 3. 상수 열 삭제
integ_df.drop(columns=constant_columns, inplace=True)

# 4. 삭제된 열의 이름 반환
print(f"Deleted columns: {constant_columns}")

integ_df.info()

integ_df.shape

# 숫자형 데이터만 선택
numeric_df = integ_df.select_dtypes(include=[float, int])

# 상관관계 계산
correlation_matrix = numeric_df.corr()

# 상관관계 행렬 출력
print(correlation_matrix)

!pip install seaborn

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Heatmap')
plt.show()

# 상관계수가 0.9 이상인 피처 식별
upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))
high_corr_features  = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]

print(f"Number of features with correlation higher than 0.9: {len(high_corr_features)}")
print(f"Features with high correlation: {high_corr_features}")

# 상관계수가 높은 피처 삭제
integ_df_reduced = integ_df.drop(columns=high_corr_features)
high_corr_features

integ_df_reduced.head()

# 삭제할 컬럼의 리스트 확인
print(f"Columns to be deleted: {high_corr_features}")

# 삭제할 컬럼의 개수 확인
print(f"Number of columns to delete: {len(high_corr_features)}")

# df_reduced의 컬럼 개수 확인
print(f"Number of columns in df_reduced after deletion: {integ_df_reduced.shape[1]}")

# 삭제 후 컬럼 목록 확인
print(f"Columns remaining in df_reduced: {integ_df_reduced.columns.tolist()}")

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# df_reduced에서 숫자형 데이터만 선택하여 상관계수 행렬 계산
# 히트맵 생성
plt.figure(figsize=(12, 10))  # 히트맵 크기 설정
sns.heatmap(integ_df_reduced.select_dtypes(include=['float', 'int']).corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Heatmap of integ_df_reduced')
plt.show()

#스케일링 진행
from sklearn.preprocessing import StandardScaler, MinMaxScaler

scaler = StandardScaler()
numeric_df_scaled = scaler.fit_transform(numeric_df)

# 범주형 데이터 열 확인
categorical_columns = integ_df_reduced.select_dtypes(include=['object', 'category']).columns

# 범주형 데이터 열 개수 출력
print(f"Number of categorical columns: {len(categorical_columns)}")
print(f"Categorical columns: {categorical_columns.tolist()}")

# 범주형 열만 선택하여 새로운 DataFrame 생성
categorical_df = integ_df_reduced[['Equipment_Dam', 'Model.Suffix_Dam', 'Workorder_Dam',
                     'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam',
                     'Model.Suffix_AutoClave', 'Workorder_AutoClave',
                     'Chamber Temp. Judge Value_AutoClave', 'Equipment_Fill1',
                     'Model.Suffix_Fill1', 'Workorder_Fill1',
                     'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1',
                     'Equipment_Fill2', 'Model.Suffix_Fill2', 'Workorder_Fill2',
                     'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2']]

# 선택된 범주형 열의 데이터 출력
print(categorical_df.head())

# 각 범주형 열에서 고유 값의 개수와 고유 값 출력
for col in categorical_columns:
    num_unique_vals = len(integ_df_reduced[col].unique())  # 고유 값의 개수
    print(f"Column: {col}")
    print(f"Number of unique values: {num_unique_vals}")
    print(f"Unique values: {integ_df_reduced[col].unique()}\n")  # 고유 값 출력

import pandas as pd

# integ_df_reduced = process_workorder(integ_df_reduced) # 과적합 생기는 것 같아서 지워볼 예정
# integ_df_reduced.drop(columns=['Workorder_Fill1','Workorder_Fill2','Workorder_Dam','Workorder_AutoClave'], inplace=True)
integ_df_reduced

def impute_obj_type_vals(df, col_name, val_before, val_after):
    # 주어진 데이터프레임의 특정 칼럼에서 특정 값을 다른 값으로 대체하는 함수
    df[col_name] = df[col_name].apply(lambda x: val_after if x == val_before else x)
    return df

integ_df_reduced = impute_obj_type_vals(integ_df_reduced, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam', 'OK', 550.3)

integ_df_reduced = impute_obj_type_vals(integ_df_reduced, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1', 'OK', 838.6)

integ_df_reduced = impute_obj_type_vals(integ_df_reduced, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1', np.nan, 838.5)

# 'tt' 칼럼을 기준으로 train과 test로 다시 분리
train = integ_df_reduced[integ_df_reduced['tt'] == 'train'].copy()
test = integ_df_reduced[integ_df_reduced['tt'] == 'test'].copy()

# 'tt' 칼럼은 이제 필요 없으므로 삭제
train.drop(columns=['tt'], inplace=True)
test.drop(columns=['tt'], inplace=True)

# train 데이터에 원래 target 값 추가
train['target'] = train_target.values

# test 데이터에 NaN으로 채워진 target 칼럼 추가
test['target'] = float('nan')

# 결과 확인
# print("Train data shape:", train.shape)
# print("Test data shape:", test.shape)
# print(train.head())
# print(test.head())

train

normal_ratio = 2.0  # 1.0 means 1:1 ratio

df_normal = train[train["target"] == "Normal"]
df_abnormal = train[train["target"] == "AbNormal"]

print(f"  Total: Normal: {len(df_normal)}, AbNormal: {len(df_abnormal)}")

df_normal = df_normal.sample(n=int(len(df_abnormal) * normal_ratio), replace=False, random_state=RANDOM_STATE)

df_abnormal.loc[:, 'target'] = df_abnormal['target'].astype(str)
df_normal.loc[:, 'target'] = df_normal['target'].astype(str)

train_df = pd.concat([df_normal, df_abnormal], axis=0).reset_index(drop=True)
train_df.value_counts("target")

train_df['target']

# 인코딩: 'Normal'을 1로, 'Abnormal'을 0으로 변환
mapping = {'Normal': 1, 'AbNormal': 0}
train_df['target'] = train_df['target'].map(mapping)

X = train_df.drop('target', axis=1)
y = train_df['target']

# for c in sorted(X.columns.tolist()):
#   print(c)

# train test 데이터셋 분리
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.isnull().sum()

X_train

!pip install catboost
!pip install optuna

# X_train.isna().sum().sum()
y_train

from catboost import CatBoostClassifier, Pool
import optuna
import numpy as np
from sklearn.metrics import f1_score
from sklearn.model_selection import StratifiedKFold

# 모든 범주형 열의 NaN 값을 'NaN' 문자열로 대체
def preprocess_data(df):
    categorical_columns = df.select_dtypes(include=['object', 'category']).columns
    for col in categorical_columns:
        df[col] = df[col].astype(str).fillna('NaN')
    return df

# 데이터 전처리
X_train = preprocess_data(X_train)

# 범주형 변수 인덱스 찾기
categorical_features_indices = [
    i for i, col in enumerate(X_train.columns)
    if X_train[col].dtype.name in ['category', 'object']
]

# Optuna 목적 함수 정의
def objective(trial):
    # 하이퍼파라미터 탐색 공간
    iterations = trial.suggest_int('iterations', 500, 1000)
    learning_rate = trial.suggest_float('learning_rate', 1e-3, 0.3, log=True)
    depth = trial.suggest_int('depth', 4, 10)
    l2_leaf_reg = trial.suggest_float('l2_leaf_reg', 1e-2, 100.0, log=True)
    rsm = trial.suggest_float('rsm', 0.5, 1.0)
    border_count = trial.suggest_int('border_count', 32, 255)

    model = CatBoostClassifier(
        iterations=iterations,
        learning_rate=learning_rate,
        depth=depth,
        l2_leaf_reg=l2_leaf_reg,
        rsm=rsm,
        border_count=border_count,
        eval_metric='F1',
        random_seed=42,
        logging_level='Silent'
    )

    # Cross-validation 설정
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    f1_scores = []

    for train_idx, val_idx in skf.split(X_train, y_train):
        X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]

        # 데이터 Pool 생성
        train_pool = Pool(data=X_fold_train, label=y_fold_train, cat_features=categorical_features_indices)
        val_pool = Pool(data=X_fold_val, label=y_fold_val, cat_features=categorical_features_indices)

        # 모델 학습
        model.fit(
            train_pool,
            eval_set=val_pool,
            early_stopping_rounds=100,
            verbose=False
        )

        # 모델 예측
        y_pred_encoded = model.predict(X_fold_val)

        # 각 fold에 대한 F1 점수 계산
        fold_f1_score = f1_score(y_fold_val, y_pred_encoded)
        f1_scores.append(fold_f1_score)

    # 모든 fold의 F1 점수 평균 반환
    return np.mean(f1_scores)

# Optuna study 설정 및 최적화
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)

# 최적의 하이퍼파라미터 출력
print(f"Best trial: {study.best_trial.params}")

# 최적의 하이퍼파라미터로 최종 모델 학습
best_params = study.best_trial.params
model = CatBoostClassifier(
    iterations=best_params['iterations'],
    learning_rate=best_params['learning_rate'],
    depth=best_params['depth'],
    l2_leaf_reg=best_params['l2_leaf_reg'],
    rsm=best_params['rsm'],
    border_count=best_params['border_count'],
    eval_metric='F1',
    random_seed=42
)

model.fit(
    X_train,
    y_train,
    cat_features=categorical_features_indices,
    verbose=100,
    early_stopping_rounds=100
)

# 최종 모델로 예측
X_val = preprocess_data(X_val)
y_pred_encoded = model.predict(X_val)

# F1 점수 계산
f1 = f1_score(y_val, y_pred_encoded)
print(f"Final F1 score: {f1}")

# 피처 중요도 시각화
feature_importances = model.get_feature_importance()
feature_names = X_train.columns

# 중요도를 데이터프레임으로 정리
importances_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False) # 중요도 순으로 정렬

# 시각화
plt.figure(figsize=(10, 8))
sns.barplot(x='Importance', y='Feature', data=importances_df)
plt.title('Feature Importance')
plt.show()

# 모든 범주형 열의 NaN 값을 'NaN' 문자열로 대체 (X_test 포함)
# categorical_columns = test.select_dtypes(include=['object', 'category']).columns

# for col in categorical_columns:
#     test[col] = test[col].astype(str).fillna('NaN')

# exclude_columns = ['Workorder_first', 'Workorder_sec', 'Workorder_third']
# test['Workorder_third']

try:
  test.drop(columns=['target'], inplace=True)
except:
  print("target col not in test, good to go")

test = preprocess_data(test)

# 모델 예측
test_pred = model.predict(test)

test_pred

# 디코딩
mapping = {1: 'Normal', 0: 'AbNormal'}
test_pred = np.vectorize(mapping.get)(test_pred)

# submission 파일 호출
df_sub = pd.read_csv(os.path.join(ROOT_DIR, "submission.csv"))

df_sub["target"] = test_pred

# 제출 파일 저장
df_sub.to_csv(os.path.join(ROOT_DIR, "submission_0828.csv"), index=False)